<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Interpretable Bioacoustic Classifiers</title>
  <meta name="description" content="Interpretable bioacoustic classifiers">
  <meta name="author" content="Kieran Gibb">
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="assets/css/normalize.css">
  <link rel="stylesheet" href="assets/css/skeleton.css">
  <link rel="icon" type="image/png" href="assets/images/favicon.png">
  </style>
</head>
<body>
  <div class="container">
    <header style="text-align: center">
      <h1 style="font-size: 4.5rem">Interpretable Bioacoustic Classifiers</h1>
      <h3 style="font-size: 3.5rem">Modelling temporal shift-invariance in self-supervised generative models improves accuracy and interpretability of species detection in weakly labelled soundscape recordings</h3>
      <p><a href="https://orcid.org/0000-0003-1468-547X">Kieran A. Gibb</a></p>
      <p><a href="http://orcid.org/0000-0001-7093-2143">Alice Eldridge</a> | <a href="http://orcid.org/0000-0002-6061-2231">Aisha Lawal Shuaibu</a> | <a href="http://orcid.org/0000-0001-5605-6626">Ivor J. A. Simpson</a></p>
      <p><a href="https://www.biorxiv.org/content/10.64898/2025.12.09.693207v1">[Paper]</a> | <a href="https://github.com/m4gpi/interpretable_bioacoustic_classifiers">[Code]</a></p>
    </header>

    <main>
      <section class="row">
        <h2>Abstract</h2>
        <p>
          Realising the potential for acoustic monitoring to deliver biodiversity insight at scale requires new approaches to the automated analysis of PAM recordings that are trustworthy as well as cost-effective. Discriminative models trained on annotated species data are gaining popularity but are labour intensive, notoriously opaque and biased. Self-supervised generative models such as Variational Autoencoders (VAE) offer great potential for learning compact yet expressive representations of data and can provide a strong prior for use in downstream discriminative tasks such as species detection while being intrinsically interpretable.
        </p>
        <p>
          We propose and evaluate a novel modification to the VAE learning algorithm that models <em>intra-frame shift-invariance</em>. We demonstrate that this modification (SIVAE) provides representations that are more interpretable, consistent and yield good performance on few-shot learning task with very weakly labels. Species-specific <em>Linear classifiers</em> were trained to predict the presence or absence of over 150 avian and anuran species using SIVAE representations of 60s long audio samples. A simple attention mechanism guides the selection of relevant frames to identify the most relevant timestep(s), while L1 regularisation sparsifies the linear model weights to perform species-specific feature selection.
        </p>
        <p>
         Whilst demonstrated in terrestrial recordings, the approach is transferable to marine, freshwater, and soil habitats. These innovations set the path for trustworthy, data and time-efficient tools to support solid ecological inference from large-scale passive acoustic monitoring surveys.
        </p>
      </section>

      <section>
        <div class="row">
          <div class="twelve columns">
            <h2>Disentangling Intra-frame Shift</h2>
          </div>
        </div>
        <div class="row">
          <div class="twelve columns">
            <p>
              The absolute temporal position of events that lie within that frame (e.g. the onset of a particular bird vocalisation) are arbitrary when frames are segmented automatically (as is the norm). However, during training the classic VAE algorithm finds the most parsimonious compressed latent representation that minimises the difference between input and generated spectrograms. The results is spurious information (such as distance from frame start to signal onset) carries as much weight as more ecologically meaningful information (such as the distance between peaks or nuances of spectral morphology which represent acoustic traits of a given species).
            </p>
          </div>
        </div>
        <div class="row">
          <div class="twelve columns">
            <figure>
              <img src="assets/images/disentangling_intra_frame_shift.png" alt="intra_frame_shift" style="width: 100%">
              <figcaption>
                <b>Figure 1:</b> <b>(B)</b> A bird call interpolated into a generic soundscape background and shifted in time \(\delta\) by linear interpolation in time <b>(A)</b>. Taking the 1st derivative of the frame representation \(\mathbf{z}\) with respect to the shift across the latent space highlights its inconsistency for a classic VAE <b>(C)</b>. The shift-invariant soundscape VAE is robust to such a perturbation <b>(D)</b>.
              </figcaption>
            </figure>
          </div>
        </div>
        <div class="row">
          <div class="twelve columns">
            <p>
              We explicitly disentangle absolute position within a frame by training a <em>shift prediction network</em> which predicts the intra-frame shift \(\delta \in [-1, 1]\) of the frame contents, with respect to a learned canonical timeline. Consistency is enforced by a 2-way cross-decoding across paired instances, where each instance is a translation of an input frame. A periodic boundary condition is applied and we regularise by learning to specify a zero-centered prior distribution.
            </p>
          </div>
        </div>
        <div class="row">
          <div class="twelve columns" style="text-align: center">
            <p>$$\mathcal{L}_{\text{align}} = -\mathcal{N}(\hat{\boldsymbol{\delta}}; \mathbf{0}, \sigma_{\mathcal{T}}) = \sum_{t=0}^T \frac{1}{2}\bigg(\log(\sigma^2_{\mathcal{T}}) + \frac{\hat{\delta}_t^2}{\sigma^2_{\mathcal{T}}} \bigg)$$</p>
          </div>
        </div>
      </section>

      <section>
        <div class="row">
          <div class="twelve columns">
            <h2>Model Architecture</h2>
          </div>
          <p>
            SIVAE is pre-trained to embed and reconstruct log mel spectrograms. Downstream linear classifiers are fit to samples from the learned posterior to predict weak presence / absence labels with a learned per-species gated attention mechanism to identify the most relevant frames \(t \in T\) in each sample.
          </p>
          <div class="twelve columns">
            <figure>
              <img src="./assets/images/siv-architecture.png" alt="", style="width: 100%">
              <figcaption>
                <b>Figure 2</b>: <b>(Top)</b> Encoding architecture overview. Spectrograms \(\mathbf{x}_{0, \ldots, T}\) are encoded into intra-frame shift-invariant latent representations, \(\mathbf{z}_{0, \ldots, T}\), with a corresponding shift \(\hat{\boldsymbol{\delta}}_{0, \ldots, T}\) for each frame. These representations are learned in a fully self-supervised manner; subsequently, classifiers can be trained using \(\mathbf{z}\) as a feature. <b>(Bottom)</b> Decoding architecture overview. Samples \(\bar{\mathbf{z}}\) are drawn from the average Gaussian distribution using the parametrisation trick. The decoder maps the sample \(\bar{\mathbf{z}}_t\) to approximate each input spectrogram, both a full contiguous clip and independently translated frames. During decoding each frame representation is duplicated for each target and is independently shifted in time by applying a translation \(\mathcal{T}(..., \hat{\boldsymbol{\delta}})\).
              </figcaption>
            </figure>
          </div>
        </div>
      </section>

      <section>
        <div class="row">
          <div class="twelve columns">
            <h2>Generative Species Representations</h2>
          </div>
        </div>
        <div class="row">
          <div class="twelve columns">
            <p>
              Using the generative model, we can inspect the basis of a detection by appling classifier weights as an affine transformation in the latent space, generating novel samples from regions predictive of each species. Specifically, we define a species presence transformation \(\mathcal{S}_j(\mathbf{z}_k)\):
            </p>
          </div>
        </div>
        <div class="row">
          <div class="four columns" style="text-align: center">
            <p>
              $$
              d_{j,k} = \frac{W_j^T\mathbf{z}_k + b}{||W_j||},
              $$
            </p>
          </div>
          <div class="four columns" style="text-align: center">
            <p>
              $$
              \bar{W}_j = \frac{W_j}{||W_j||},
              $$
            </p>
          </div>
          <div class="four columns" style="text-align: center">
            <p>
              $$
              \mathcal{S}_j(\mathbf{z}_k) = \mathbf{z}_k + (d_{j,k} + \delta)\bar{W}_j
              $$
            </p>
          </div>
        </div>
        <div class="row">
          <div class="twelve columns">
            <p>
              where \(\mathbf{z}_k\) is a sample from an appropriate starting region of the latent space, \(d_{j,k}\) is the signed distance to the hyperplane for species \(j\), \(\bar{W}\) is the direction normal to the hyperplane and \(\delta\) is a tunable distance parameter beyond the decision boundary controlling the amplitude of the signal in the generated spectrogram. We set \(\mathbf{z}_k\) by taking the average for each habitat resulting in a habitat-specific silent background. The final result is decoded \(\mathcal{D}(\mathcal{S}_j(\mathbf{z}_k))\) providing a spectogram illustrating the predictive factors for each species.
            </p>
          </div>
        </div>
        <div class="row">
          <div class="four columns">
            <p><b>(1)</b> Select a species:</p>
            <select id="species-select" name="species" id="species-select" style="width: 100%">
              <optgroup label="Sounding Out (UK)">
                <option value="Eurasian Blackbird">Eurasian Blackbird</option>
                <option value="European Robin">European Robin</option>
                <option value="Ring-necked Pheasant">Ring-necked Pheasant</option>
                <option value="Winter Wren">Winter Wren</option>
                <option value="Common Wood-Pigeon">Common Wood-Pigeon</option>
                <option value="Carrion Crow">Carrion Crow</option>
              </optgroup>
              <optgroup label="Sounding Out (Ecuador)">
                <option value="Chestnut-backed Antbird">Chestnut-backed Antbird</option>
                <option value="Pallid Dove">Pallid Dove</option>
                <option value="Yellow Tyrannulet">Yellow Tyrannulet</option>
                <option value="White-bearded Manakin">White-bearded Manakin</option>
              </optgroup>
              <optgroup label="RFCX (Puerto Rico)">
                <option value="Bananaquit">Bananaquit</option>
                <option value="Red-legged Thrush">Red-legged Thrush</option>
                <option value="Elfin Woods Warbler">Elfin Woods Warbler</option>
                <option value="Black-whiskered Vireo">Black-whiskered Vireo</option>
                <option value="Puerto Rican Woodpecker">Puerto Rican Woodpecker</option>
              </optgroup>
            </select>
            <br/>
            <p><b>(2)</b> Move across the hyperplane:</p>
            <input id="delta-slider" type="range" min="1" max="30" step="1" value="1"  style="width: 100%"/>
          </div>
          <div class="eight columns">
            <p><b>(3)</b> Compare generated spectrogram with a typical example:</p>
            <figure>
              <img id="reconstruction" src="assets/images/frames/Eurasian Blackbird_0.png" alt="" style="width: 100%">
              <figcaption>
                <b>Figure 3:</b> <b>(Left)</b> a real example call typical of each species. <b>(Right)</b> a decoded generative species representation highlights what sounds were used for detecting a species.
<b></b>
              </figcaption>
            </figure>
          </div>
        </div>
        <script>
          let select = document.getElementById("species-select")
          let slider = document.getElementById("delta-slider")
          let img = document.getElementById("reconstruction")

          function render () {
            let species = select.options[select.selectedIndex].value
            let delta = slider.value
            img.src = "assets/images/frames/" + species + "_" + delta + ".png"
          }

          select.addEventListener("change", render, false)
          slider.addEventListener("change", render, false)

          render()
        </script>
      </section>

      <section>
        <div class="row">
          <div class="twelve columns">
            <h2>Species Detection</h2>
          </div>
        </div>
        <div class="row">
          <div class="twelve columns">
            Quantitative experiments show our approach was both effective in terms of discriminative performance, achieving the highest area under the ROC (0.92) and Top-1 accuracy (0.67), outperforming BirdNET V2.4 (0.89, 0.52) while achieving good average precision (AP) despite using very weak presence annotations for each minute of audio.
          </div>
        </div>
        <div class="row">
          <div class="twelve columns">
            <figure>
              <img src="assets/images/so_violin.png" alt="" style="width: 100%">
              <figcaption>
                <b>Figure 4:</b> Area under the ROC curve (auROC) and average precision (AP) score distributions across out-of-the-box BirdNet V2.4 and both VAE variants. Full distributions show across species the auROC is approximately equivalent with BirdNET and across the VAE and SIVAE for the full dataset, while our models typically perform slightly better on examples with high occlusions. Both VAE variant representations provide less precision than BirdNET in the SO dataset. Introducing shift-invariance results in a small improvement in the UK and a drastic improvement in RFCX bird dataset, while we see a slight drop in precision in Ecuadorian soundscapes.
              </figcaption>
            </figure>
          </div>
        </div>
        <div class="row">
          <div class="twelve columns">
            <figure>
              <img src="assets/score_table.png" alt="" style="width: 100%">
              <figcaption>
                <b>Table 1:</b> Area under the ROC (auROC), mean average precision (mAP) and top-1 accuracy scores for species detection models trained on latent space representations for each model variant alongside BirdNET and Perch. Scores are averaged across the entire species community over 3 different training runs for each dataset. Comparisons are made with both out-of-the-box BirdNET V2.4 and the best results from <a href="https://www.nature.com/articles/s41598-023-49989-z">Ghani et al. (2023)</a> where BirdNet and Perch were fine-tuned on a few-shot learning task. * indicates ``strong labels" where bounding boxes were used to delineate the call in the training data. Size denotes the dimensionality of the embedding for the corresponding temporal resolution in seconds."
              </figcaption>
            </figure>
            <p></p>
          </div>
        </div>
      </section>
    </main>

    <br/>

    <footer>
      <div class="row">
        <h4>Acknowledgements</h4>
        <p>
          Many thanks to my supervisors Ivor Simpson, Alice Eldridge and Chris Sandom, and my colleagues in the LILI lab at the University of Sussex. My funders, the Leverhulme Trust and the be.AI scholarship programme.
        </p>
        <div>
          <img src="assets/images/sussex-logo.png" alt="" width="150" style="float: left">
          <img src="assets/images/acknowledgements.png" alt="acknowledgements" style="float: right">
        </div>
      </div>
    </footer>
  </div>
</body>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
</html>
